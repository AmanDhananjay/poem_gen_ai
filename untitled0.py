# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13xFEqf1qjaBdYJ67curqdzCm-6OLucaQ
"""

# !pip install transformers

import pandas as pd
import numpy as np
import re
from transformers import GPT2Tokenizer, TextDataset
import os

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/PoetryFoundationData.csv')
data_frame = pd.DataFrame(df)

df

poems = df['Poem'].tolist()
text_data = "\n".join(poems)

with open("poems.txt", "w") as file:
    file.write(text_data)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="poems.txt",
    block_size=128
)

from transformers import GPT2LMHeadModel, DataCollatorForLanguageModeling

model = GPT2LMHeadModel.from_pretrained('gpt2')
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./poem_model",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=2,
    save_steps=1000,
    save_total_limit=2
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset
)

trainer.train()
print("train compleat")
trainer.save_model()
print("save model")


def generate_poem(model, tokenizer, prompt, max_length=50):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1)
    generated_poem = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_poem

model_path = "./poem_model"
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

prompt = "The sky is filled with"
print(generate_poem(model, tokenizer, prompt))